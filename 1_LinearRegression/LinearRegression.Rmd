---
title: "Linear regression"
author: "Louis-Marc Mercier"
date: "January 2016"
output: html_document
   
---
## Introduction
 Linear regression is a powerful tool and can be used for many purposes. For example, one can make some predictions and study the strenght of the relationship between some variables. More precisely, the idea behind linear regression is to model the relationship between a scalar dependent variable and one or more explanatory variables.  For only one explanatory variable, we call it *simple* linear regression. If we have more than one explanatory variables, we call it *multivariate* linear regression.   
 
 
 **Packages Used** :glmnet, ElemStatLearn, MASS, broom, ggplot2, visreg, scatterplot3d

## Example: Height and weight of people
Let's do an example to understand this concept. Imagine that you are in a class of 25 students. You want to be able to *guess* (approximate) the weight of a given student. However, some of the students might not want to reveal this information. So, you select a group of 5 students which you think that are *representative* of the whole class. You obtain the following table:

|     Weight(kg)   |  Name |
|:-------------:|:------:|
|    70        | Anabelle |
|    65       |   Boris |
|    55        |  Carl |
|    70       |  Doris |
|    70       |  Edward |

Now, if I give you the name of a student in the class, how would you estimate his weight? Well, an intuitive solution would be to calculate his weight with the arithmetic mean of the weights of our sample. By doing this, we suppose that each student is equally representative for our model. Otherwise, we could make our prediction with a weighted-average. Now, let's explicitate this idea mathematically. Our model does not use any information on any individual, so our prediction will be in the following form $\hat{y}=\hat{\beta_{0}}$ where $\hat{\beta_{0}}\in\mathbb{R}$. By estimating with a scalar for all the individuals, we know that there will be an error for each individual. In other terms, we have that $y_{i}=\hat{y}_{i}+e_{i}=\hat{\beta_{0}}+e_{i}$. The idea is to minimize all the errors ($e_{i}$), i.e minimize $e_{i}=y_{i}-\hat{y}_{i}$ for all $i$. One of the way to do that is to minimize $\sum_{i=1}^{n}e_{i}^{2}$ ($n$: number of observations). Don't forget that the errors can be negative or positive. So by squaring them, we avoid some cancellations that could happen in a sum of errors. So our purpose is to minimize the following sum:
$$S(\beta_{0})=\sum_{i=1}^{n}(y_{i}-\beta_{0})^{2}$$
To find the minimum, let's set $\frac{dS}{d\beta_{0}}=0$. We obtain:
$$\frac{dS}{d\beta_{0}}=-2\sum_{i=1}^{n}(y_{i}-\beta_{0})=0\Rightarrow \hat{\beta_{0}}=\bar{y}$$
Which is what we obtained precedently! We can make sure that it is a minimum if $\frac{d^{2}S}{d\beta_{0}^2}>0$ for $\hat{\beta_{0}}$:
$$\frac{d^{2}S}{d\beta_{0}^2}=2n>0 \quad\forall n\ge 1$$
This implies that the function is strictly convex. Consequently, this minimum is a global minimum.

Now, what if we have more informations? Let's say that the students of our sample agree to give their weights for some amount of money. Here is what we have now:

| Height (cm)| Weight (kg)   |  Name |
|:--------:|:----------:|:------:|
|  172 |  70 | Anabelle |
| 165 |    65   |   Boris |
| 155 | 55 |  Carl |
| 180 |    70   |   Doris |
| 187 | 70 |  Edward |

We now dispose of one explanatory variable which is the height. Logically, the taller a person is, the heavier he should be. Our new model will be of the following form:
$$y_{i}=\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i} $$
We can interpret $\beta_{0}$ as the weight of a person with no height and $\beta_{1}$ as the variation in the weight for an increasement of one centimeter in height. Strangely, our model tells us that a person with no height could have a weight...but we know that this is not possible. However, our interest is to do **predictions**! We want to be able to estimate the weight of a given human being and we know that we will never have a human being without any height. Now, we have to minimize:
$$S(\beta_{0},\beta_{1})=\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}$$
We now have to set $\frac{dS}{d\beta_{0}}=0$ and $\frac{dS}{d\beta{1}}=0$, which give:
$$\frac{dS}{d\beta_{0}}=-2\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})\quad (i)$$
$$\frac{dS}{d\beta_{1}}=-2\sum_{i=1}^{n}x_{i}(y_{i}-\beta_{0}-\beta_{1}x_{i})\quad (ii)$$
The equation (i) gives:
$$\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1}\bar{x}$$
If we substitute it back in the equation (ii), we obtain:
$$
\hat{\beta_{1}}=\frac{\sum\limits_{i=1}^n x_{i}(y_{i}-\bar{y})}{\sum\limits_{i=1}^n x_{i}(x_{i}-\bar{x})}=\frac{\sum\limits_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum\limits_{i=1}^n (x_{i}-\bar{x})^2} \quad \Bigg(\text{where } \bar{x}=\sum\limits_{i=1}^n \frac{x_{i}}{n}\Bigg)
$$
We can prove with *Multivariate Calculus* that $(\hat{\beta}_{0},\hat{\beta}_{1})$ is a minimum, but it is a particular case of a more general theorem that will be prove later.

## Example on R: weight of students
Let's add some complexity to our previous problem. We now dispose of the age of the students in the sample.

| Height (cm)| Age (years)   | Weight (kg)  | Name |
|:--------:|:----------:|:------:|:------:|
|  172 |  31 | 70 |Anabelle |
| 165 |    31   |   65 | Boris |
| 155 | 30 |  55 | Carl |
| 180 |    35   |    70  | Doris |
| 187 | 28 |  70 |  Edward |

```{r, message=FALSE, warning=FALSE}
library(scatterplot3d)

Height=c(172,165,155,180,187)
Age=c(31,31,30,35,28)
Weight=c(70,65,55,70,70)

df = data.frame(Height,Age,Weight)
S3D = scatterplot3d(Height, Age, Weight, pch=16, highlight.3d=TRUE,
type="h", main="Weight,age and height of students with vertical lines and regression plane")

fit <- lm(Weight ~ Height+Age)
print(fit$coefficients)
S3D$plane3d(fit)
```

## Multivariate Linear Regression: Notation
The example that has been treated is interesting. However, we need some formalization to exploit the power of the linear regression. Let's generalize the concept of the *simple* linear regression to the *multivariate* linear regression.

Let's consider a linear regression model with $k$ explanatory variables $x_{1},x_{2},...,x_{k}$ and $n$ observations. The model can be written in the following way:
$$y_{1}=\beta_{0}+\beta_{1}x_{11}+\beta_{2}x_{12}+...+\beta_{k}x_{1k}+\varepsilon_{1}$$
$$y_{2}=\beta_{0}+\beta_{1}x_{21}+\beta_{2}x_{22}+...+\beta_{k}x_{2k}+\varepsilon_{2}$$
$$\vdots$$
$$y_{n}=\beta_{0}+\beta_{1}x_{n1}+\beta_{2}x_{n2}+...+\beta_{k}x_{nk}+\varepsilon_{n}$$
If we want to link this with the preceding problem, the $y_{i}$ corresponds to the weight of the $i$-th person in our sample and the explanatory variables are some information on them (like their height,age,eye color,etc.). Luckily, these equations can be put in a matricial form:
$$\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\mathbf{\varepsilon} \quad \text{where}$$
$$ 
\mathbf{y}=
\begin{bmatrix}
  y_{1}  \\ 
  y_{2}  \\ 
    \vdots \\ 
  y_{n}
   \end{bmatrix}
  , \mathbf{X}=\begin{bmatrix}
  1 & x_{11} &...& x_{1k} \\ 
  1 & x_{21} &...& x_{2k} \\ 
  \vdots &\vdots & \ddots & \vdots \\ 
  1 & x_{n1} & ... & x_{nk}
  \end{bmatrix}
  ,\boldsymbol{\beta}=
\begin{bmatrix}
  \beta_{0}  \\ 
  \beta_{1}  \\ 
    \vdots \\ 
  \beta_{k}
   \end{bmatrix} \quad \text{ and }
   \
  \mathbf{\varepsilon}=
\begin{bmatrix}
  \varepsilon_{1}  \\ 
  \varepsilon_{2}  \\ 
    \vdots \\ 
  \varepsilon_{n}
   \end{bmatrix}
$$
Here are a few important points that we have to know:

* $\mathbf{y}$ is a random vector.
* $\mathbf{X}$ is a matrix $n\times(k+1)$ and is known as design matrix. The $i$-th row of this matrix is $\mathbf{x}_{i}^{\top}=(1,x_{i1},...,x_{ik})$.
* $\boldsymbol{\beta}$ is the vector of the unknown parameters of the model.
* $\mathbf{\varepsilon}$ is the vector of the errors.
* To avoid some problems of multicollinearity, the matrix $\mathbf{X}$ has to be a full rank-$(k+1)$ matrix. If the design matrix is not full-rank, $\mathbf{X}^{\top}\mathbf{X}$ is singular and $\boldsymbol{\beta}$ is not uniquely defined.

## Multivariate Linear Regression: Ordinary Least Squares
The parameters estimates $\hat{\beta_{0}},\hat{\beta_{1}},...,\hat{\beta_{k}}$ are knowned as ordinary least squares estimates. We obtain them by minimizing the following expression:
$$S(\boldsymbol{\beta})=\sum_{i=1}^{n}\varepsilon_{i}^{2}=\mathbf{\varepsilon}^{\top}\mathbf{\varepsilon}=(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^{\top}(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})=\mathbf{y}^{\top}\mathbf{y}-2\boldsymbol{\beta}^{\top}\mathbf{X}^{\top}\mathbf{y}+\boldsymbol{\beta}^{\top}\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\beta} $$
To find the minimum, we proceed in a similar way to the one in the last example. Let's set $\frac{dS}{d\boldsymbol{\beta}}=0$:
$$\frac{dS}{d\boldsymbol{\beta}}=\frac{d}{d\boldsymbol{\beta}}[\mathbf{y}^{\top}\mathbf{y}]-2\frac{d}{d\boldsymbol{\beta}}[\boldsymbol{\beta}^{\top}\mathbf{X}^{\top}\mathbf{y}]+\frac{d}{d\boldsymbol{\beta}}[\boldsymbol{\beta}^{\top}\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\beta}]=-2\mathbf{X}^{\top}\mathbf{y}+2\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\beta}=0 $$
The result is the following linear system of eqations:
$$\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\beta}=\mathbf{X}^{\top}\mathbf{y} \quad (iii)$$
Our interest is to isolate $\boldsymbol{\beta}$ in this equation. However, is $\mathbf{X}^{\top}\mathbf{X}$ is inversible? If so, the answer would be:
$$\hat{\boldsymbol{\beta}}=(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}$$
In fact, $\mathbf{X}^{\top}\mathbf{X}$ is invertible! Indeed, $\mathbf{X}^{\top}\mathbf{X}$ is a full-rank matrix (by hypothesis) and a semi positive definite matrix (**do it as an exercise**). So, the matrix is inversible. This leads us to the conclusion that the estimator by ordinary least squares should always be computable if $\mathbf{X}$ is a full rank matrix. However, inversing a matrix is an unstable process numerically. The best option is to solve the linear system $(iii)$. **DO NOT COMPUTE THE INVERSE**. If the matrix on the left side of the equation has eigenvalues near of zero, the matrix will be computably hard to inverse. 

In fact, the function $S(\boldsymbol{\beta})$ is a convex function. So, our minimum is a global minimum. It is pretty straightforward to show:
$$\frac{d^{2}S}{d\boldsymbol{\beta}^{\top}d\boldsymbol{\beta}}=2\mathbf{X}^{\top}\mathbf{X}$$
Convex optimization theory deems that if our function $S(\boldsymbol{\beta})$ is twice differentiable (it is!), then our fonction is convex iff:

* $dom(S(\boldsymbol{\beta}))$ is a convex set.
* $\mathbf{\nabla}^{2}S(\boldsymbol{\beta})$ is positive semi-definite for all $\boldsymbol{\beta}\in dom(S(\boldsymbol{\beta}))$

These conditions are respected. Firstly, we have that $dom(S(\boldsymbol{\beta}))=\mathbf{R}^{n}$, which is a convex set. Secondly, we know that $\mathbf{X}^{\top}\mathbf{X}$ is positive semi definite. So $S(\boldsymbol{\beta})$ is a convex function and consequently, $\hat{\boldsymbol{\beta}}$ is a global minimum.

## Multivariate Linear Regression: Gauss-Markov properties
The ordinary least squares estimate is the best unbiased linear estimate under the following conditions:

* $\mathbb{E}(\mathbf{\varepsilon})=\mathbf{0}$
* $\mathbb{E}(\mathbf{\varepsilon}\mathbf{\varepsilon}^{\top})=\sigma^{2}\mathbf{I}$

## Multivariate Linear Regression: Important properties

* We define the residuals as $\mathbf{e}=y-\hat{y}$. There is a link between the errors, the residuals and the response variable:
$$\mathbf{e}=(\mathbf{I}-\mathbf{H})\mathbf{y}=\mathbf{M}\mathbf{y}=\mathbf{M}\mathbf{\varepsilon}$$
where $\mathbf{H}=\mathbf{X}(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}$ is knowned as the hat-matrix, $\mathbf{I}$ is the identity matrix of size $n\times n$ and $\mathbf{M}=\mathbf{I}-\mathbf{H}$. Here are some important properties:

    * $\mathbf{M}$ and $\mathbf{H}$ are symetrics and idempotents matrices.
    * $\mathbf{M}\mathbf{X}=0$ 
    

* If the Gauss-Markov conditions are respected, we have that:

    * $\mathbb{E}(\mathbf{y})=\mathbf{X}\mathbf{\beta}$
    * Cov$(\mathbf{y})=\sigma^{2}\mathbf{I}$ and Cov$(\hat{\mathbf{y}})=\sigma^{2}\mathbf{H}$
    * $\mathbf{E}(\mathbf{e}\mathbf{e}^{\top})=\sigma^{2}\mathbf{M}=\sigma^{2}(\mathbf{I}-\mathbf{H})$
    * Var$(e_{i})=\sigma^{2}m_{ii}=\sigma^{2}(1-h_{ii})$
    * $0\le h_{ii}\le 1$
    * $\mathbb{E}(\hat{\boldsymbol{\beta}})=\mathbf{\beta}$ and Cov$(\beta)=\sigma^{2}(\mathbf{X}^{\top}\mathbf{X})^{-1}$
    * $s^{2}=\frac{\mathbf{e}^{\top}\mathbf{e}}{n-k-1}=\frac{RSS}{n-k-1}$ is an unbiased and convergent estimator of $\sigma^{2}$.
    
* The residuals vector is orthogonal to the fitted values vector and the residuals vector is orthogonal to the design matrix. Another way to express this is:

    * $\mathbf{X}^{\top}\mathbf{e}=0$
    * $\hat{\mathbf{y}}\mathbf{e}=0$
    
Let's denote the column vectors of $\mathbf{X}$ by $\mathbf{x}_{0},\mathbf{x}_{1},...,\mathbf{x}_{k}$ with $\mathbf{x}_{0}=\mathbf{1}$. These vectors span a subspace of $\mathbb{R}^{n}$ that we will call the column space of $\mathbf{X}$. The vector $\mathbf{y}$ is orhtogonally projected onto the hyperplane spanned by the explanatory variables $x_{1},x_{2},...,x_{k}$. The hat matrix $\mathbf{H}$ computes the orthogonal projection and because of this, it is sometime call the projection matrix. Proving these properties are good exercises.


```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(png)
library(grid)
```
<center>
```{r fig.width=4.5, fig.height=4.0, message=FALSE, cache=FALSE, echo=FALSE}
img <- readPNG("figures/projection.png")
 grid.raster(img)
```
</center>
(Figure from page 46 of ESL)  

## Computation of $\boldsymbol{\beta}$

For the following methodology, if you have trouble to understand some explanations, I recommand you do read about [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) by clicking on the link. We know that $\mathbf{X}^{\top}\mathbf{X}$ is a symetric and positive semi definite matrix. However, it may be a dense matrix. Here at the steps to compute $\boldsymbol{\beta}$:

1. Factor $\mathbf{X}^{\top}\mathbf{X}=\mathbf{(QR)}^{\top}\mathbf{(QR)}=\mathbf{R}^{\top}\mathbf{Q}^{\top}\mathbf{Q}\mathbf{R}=\mathbf{R}^{\top}\mathbf{R}$ where $\mathbf{R}$ is an upper triangular matrix $\big( 2n(k+1)^{2} \text{flops}\big)$.
2. Solve $\mathbf{R}^{\top}\mathbf{R}\boldsymbol{\beta}=\mathbf{R}^{\top}\mathbf{Q}^{\top}\mathbf{y} \Leftrightarrow \mathbf{R}\boldsymbol{\beta}=\mathbf{Q}^{\top}\mathbf{y}$:
    + Form $\mathbf{d}=\mathbf{Q}^{\top}\mathbf{y}$ $\big(2n(k+1)\text{ flops}\big)$.
    + Solve $\mathbf{R}^{\top}\boldsymbol{\beta}=\mathbf{d}$ to find $\boldsymbol{\beta}$ $\big((k+1)^2\text{ flops}\big)$.

So, the total number of operations is of the order of $2n(k+1)^{2}$ flops.

## Example on R: airquality
As an example, consider the data from a 1973 study examining the daily air quality measurements in New York. This code is reproductible. So, I encourage you to try to execute this code on your computer. You will need the package "visreg" developed by Patrick Breheny and Woodrow Burchett. You can find their article named [Visualization of Regression Models Using visreg](http://myweb.uiowa.edu/pbreheny/publications/visreg.pdf) by clicking on the link. Here is a brief description of this package:

> The package provides several options for visualizing models with interactions,
including lattice plots, contour plots, and both static and interactive perspective plots. The
implementation of the package is designed to take full advantage of R's generic functions, allowing a consistent interface for visualizing not only linear models, but generalized linear models, proportional hazards models, generalized additive models, robust regression models, and more. 
- From Visualization of Regression Models Using visreg (2013)


```{r, results='hide', message=FALSE, warning=FALSE}
library(visreg)
data("airquality")
head(airquality)
```

```{r, fig.height = 2, results="hide" }
fit <- lm(Ozone ~ Solar.R + Wind + Temp, data = airquality)
summary(fit)
par(mfcol=c(1,3))
visreg(fit)
```

## Another example on R: prostate
This is a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. 
```{r, message=FALSE, warning=FALSE}
## Prostate data
require(glmnet)
require(ElemStatLearn)
library(MASS)
library(broom)
library(ggplot2)
data(prostate)
prostate <- prostate[,-ncol(prostate)]
head(prostate)
fit2 <- lm(lpsa~.,prostate)
```

The coefficients of $\mathbf{\beta}$ are the following:
```{r, fig.height = 2}
fit2$coefficients
```


## References

* Regression Analysis: Theory, Methods, and Applications.,
  Ashish Sen & Muni Srivastava,
  1990
* The Elements of Statistical Learning.,
  Trevor Hastie, Robert Tibshirani & Jerome Friedman,
  2009

